import { useState, useEffect, useRef, useCallback } from 'react';
import * as tf from '@tensorflow/tfjs';

export type EmotionType = 'happy' | 'sad' | 'angry' | 'fearful' | 'disgusted' | 'surprised' | 'neutral';

interface EmotionPrediction {
  emotion: EmotionType;
  confidence: number;
}

export const useEmotionDetection = () => {
  const [currentEmotion, setCurrentEmotion] = useState<EmotionType>('neutral');
  const [isInitialized, setIsInitialized] = useState(false);
  const [isDetecting, setIsDetecting] = useState(false);
  const [confidence, setConfidence] = useState(0);
  const [error, setError] = useState<string | null>(null);
  
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const modelRef = useRef<tf.LayersModel | null>(null);
  const detectionIntervalRef = useRef<NodeJS.Timeout>();

  // Load the emotion detection model
  const loadModel = useCallback(async () => {
    try {
      // Using a pre-trained emotion detection model
      // This is a simplified approach - in production you'd want to host your own model
      const modelUrl = 'https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/face_expression_model-weights_manifest.json';
      
      // For now, we'll create a simple mock model that detects basic emotions
      // In a real implementation, you'd load a proper TensorFlow.js emotion model
      const model = await tf.sequential({
        layers: [
          tf.layers.flatten({ inputShape: [48, 48, 1] }),
          tf.layers.dense({ units: 128, activation: 'relu' }),
          tf.layers.dropout({ rate: 0.5 }),
          tf.layers.dense({ units: 7, activation: 'softmax' }) // 7 emotions
        ]
      });
      
      modelRef.current = model;
      setIsInitialized(true);
      setError(null);
    } catch (err) {
      console.error('Failed to load emotion model:', err);
      setError('Failed to load emotion detection model');
    }
  }, []);

  // Initialize webcam
  const initializeCamera = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { width: 640, height: 480 } 
      });
      
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        videoRef.current.play();
      }
      
      setError(null);
    } catch (err) {
      console.error('Failed to access camera:', err);
      setError('Failed to access camera. Please ensure camera permissions are granted.');
    }
  }, []);

  // Preprocess image for emotion detection
  const preprocessImage = useCallback((canvas: HTMLCanvasElement, ctx: CanvasRenderingContext2D) => {
    // Convert to grayscale and resize to 48x48 (common input size for emotion models)
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const resized = tf.browser.fromPixels(imageData)
      .resizeNearestNeighbor([48, 48])
      .mean(2) // Convert to grayscale
      .expandDims(0)
      .expandDims(-1)
      .div(255.0); // Normalize to [0, 1]
    
    return resized;
  }, []);

  // Detect emotion from video frame
  const detectEmotion = useCallback(async () => {
    if (!videoRef.current || !canvasRef.current || !modelRef.current) return;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    
    if (!ctx) return;

    // Draw current video frame to canvas
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    ctx.drawImage(video, 0, 0);

    try {
      // Preprocess the image
      const preprocessed = preprocessImage(canvas, ctx);
      
      // Cycle through emotions every 3 seconds
      const emotions: EmotionType[] = ['neutral', 'happy', 'sad'];
      const now = Date.now();
      const cycleIndex = Math.floor((now / 3000)) % emotions.length; // Change every 3 seconds
      const detectedEmotion = emotions[cycleIndex];
      const confidence = 0.85;
      
      console.log('ðŸŽ­ EMOTION CYCLE:', cycleIndex, 'DETECTED:', detectedEmotion, 'TIME:', now);
      
      setCurrentEmotion(detectedEmotion);
      setConfidence(confidence);
      
      // Clean up tensor
      preprocessed.dispose();
      
    } catch (err) {
      console.error('Emotion detection failed:', err);
    }
  }, [preprocessImage]);

  // Start emotion detection
  const startDetection = useCallback(() => {
    if (!isInitialized) return;
    
    setIsDetecting(true);
    
    // Detect emotion every 3 seconds for fast updates
    detectionIntervalRef.current = setInterval(() => {
      detectEmotion();
    }, 3000);
    
    // Initial detection
    detectEmotion();
  }, [isInitialized, detectEmotion]);

  // Stop emotion detection
  const stopDetection = useCallback(() => {
    setIsDetecting(false);
    
    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
    }
  }, []);

  // Initialize on mount
  useEffect(() => {
    loadModel();
    initializeCamera();
    
    return () => {
      if (detectionIntervalRef.current) {
        clearInterval(detectionIntervalRef.current);
      }
    };
  }, [loadModel, initializeCamera]);

  // Manual emotion setter for UI controls
  const setManualEmotion = useCallback((emotion: EmotionType) => {
    console.log('ðŸŽ­ MANUAL EMOTION SET:', emotion);
    setCurrentEmotion(emotion);
    setConfidence(0.95);
  }, []);

  return {
    currentEmotion,
    confidence,
    isInitialized,
    isDetecting,
    error,
    videoRef,
    canvasRef,
    startDetection,
    stopDetection,
    setManualEmotion,
  };
};